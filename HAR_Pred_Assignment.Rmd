---
title: "Practical Machine Learning Assignment"
author: "Tryfanak"
date: "August 23, 2015"
output: html_document
---

##Summary
This report describes the steps to create a model to predict how well a subject performed a specific weight lifting exercise, based on readings from a set of on-body sensors. Data was provided which linked historic sensor readings to exercises which were performed correctly, or in one of four incorrect ways. The task was to use sensor readings from additional  to predict whether those exercises were being performed correctly, or in one of the incorrect ways.

The report describes: how the data was prepared for analyis; the partioning strategy to create training, test and validation sets; how a set of features was selected for the model; the choice of algorithm; calculation of parameters; and finally the estimation of out-of-sample errors.

The model selected is a random forest, based on a reduced subset of 8 variables. This relatively simple model has an estimated out of sample error rate of 1.1%. 

The data for this assignment can be found at http://groupware.les.inf.puc-rio.br/har, and comes originally from this paper  
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. **Qualitative Activity Recognition of Weight Lifting Exercises.** Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

##Data Preparation
```{r,echo=FALSE,message=FALSE}
library(plyr);library(dplyr);library(tidyr);library(caret);library(rattle)
```  
The training data is loaded from the supplied CSV file, treating blanks and certain Excel artifacts as NA.
```{r}
dat <- read.csv("pml-training.csv", header=TRUE, na.strings = c("NA","#DIV/0!"))
```

Upon inspection, the data contains summary rows which (from the paper) refer to time window calculations which are beyond the scope of this project. These rows were removed entirely.
```{r}
dat1 <- filter( dat, new_window == "no")
```
Once the summary rows were removed, a large number of data columns contained no values except NA - these columns were dropped.
```{r}
dat2 <- dat1[, as.vector(colSums(!is.na(dat1))) > 0]
```
The remainder of the data was fully and cleanly populated, and no imputation was required to create a useable dataset.

##Data Exploration
Inspection of the data showed several ways in which the complexity of the data set could be reduced:  
- Columns related to the identity of the user were not relevant, since this would not be useful for new users  
- Columns related to the time of the reading could not used for this specific model  
These columns were dropped from the data set
```{r}
dat3 <- select(dat2, -X, -user_name, -raw_timestamp_part_1,
                 -raw_timestamp_part_2, -cvtd_timestamp, -new_window, -num_window)
```

This left `r dim(dat3)[2]-1` columns as possible predictors for the model.

Inspection of the columns for each sensor showed that certain columns were highly correlated, and were candidates for feature reduction later. The belt sensor showed this most. For example, these are the correlations between the roll_belt column and the other belt readings...
```{r}
belt <- select(dat3, contains("_belt"))
cor(belt)[,1]
```

Two data issues of concern were beyond the scope of this project to resolve, but would need to be addressed for a professional treatment  

* The original readings were collected as a stream over time, and reflect the motion of the person performing the exercise. A rigorous analysis would address them as a sequence, with a moving time window and a class for the whole window, rather than treating each row as a separate case.  
* The belt readings are clearly inconsistent between users. The differences are so large that it appears that three of the users had positioned this sensor completely differently from the other half. There may be a data transformation which could correct this, but for this assignment it was left to the model to resolve the issue. (This is reasonable for a classification algorithm, but would not work for regression).

```{r,echo=FALSE}
qplot(roll_belt,pitch_belt,colour=user_name,data=dat2)
```

In exploratory plots, several combinations of variables showed promise as predictors, because they created separation between the different ways that the exercise could be performed. This plot shows the pattern for the forearm sensors. Examples for belt and dumbbell sensors are included in the Appendix A.  

```{r,echo=FALSE}
qplot(roll_forearm, pitch_forearm,colour=classe,data=dat2)
```

##Partitioning the Data
Since the dataset provided was relatively large, it was partitioned into training, test and validation sets.

```{r}
#Partition Validation Set
set.seed(223)
trainIndex <- createDataPartition(dat3$classe, p = 0.8,list=FALSE)
supertrain <- dat3[trainIndex,]; validation <-dat3[-trainIndex,]

#Partition Training and Test sets
set.seed(3)
trainIndex <- createDataPartition(supertrain$classe, p = 0.60/0.80,list=FALSE)
train <- supertrain[trainIndex,];test <- supertrain[-trainIndex,]
```

The resulting row counts are  
  __Training (60%)__ - `r dim(train)[1]` rows, used for feature selection, and to train different types of models  
  __Test (20%)__ - `r dim(test)[1]` rows, used to estimate out-of-sample error for each model, and select final model  
  __Validation (20%)__ - `r dim(validation)[1]` rows, set aside to estimate out of sample errors for the final model

##Feature Selection
The goal of feature selection was to reduce the number of predictors used in the final model. The initial set of 52 possible features was large and would lead to a complex model, and long training times. The feature selection process sought a smaller number of key features which could predict most of differences between classes, whilst avoid overfitting.  

To speed up feature selection, several different models were run on 10% of the data, changing the seed each time to create a series of different data sets. After each model run, the varImp function was used to evaluate the importance of each feature. The models included CART, random forests and boosted trees (GBM), since these are common choices for classification models.

```{r,message=FALSE}
#Enable parallel processing to reduce runtimes
library(doParallel) 
registerDoParallel(cores=2)

#Use 5-fold cross-validation in all fitted models
controla <- trainControl(method = "cv", number=5 )
```

The most important features were stable between different model types and input data sets, while some other predictors were of consistently low importance. (For example, no model ranked data from the arm sensor in its top variables).

```{r varImp,echo=FALSE,message=FALSE}
# Take 10% of the data to test variable importance
set.seed(458)
trainIndex <- createDataPartition(train$classe, p = 0.10/0.60,list=FALSE)
smallTrain1 <- train[trainIndex,]
set.seed(9741)
trainIndex <- createDataPartition(train$classe, p = 0.10/0.60,list=FALSE)
smallTrain2 <- train[trainIndex,]
set.seed(201)
modFit1 <- train(classe~., method="gbm",data=smallTrain1,trControl=controla, verbose=FALSE)
set.seed(41)
modFit2 <- train(classe~., method="rf",data=smallTrain2, trControl=controla)
```

__Example of Top 15 predictors for Boosted Tree Model (10% of data)__
```{r,echo=FALSE,fig.height=4,fig.width=6}
plot(varImp(modFit1),15)
```

__Example of Top 15 Predictors for Random Forest Model (10% of data)__
```{r,echo=FALSE,fig.height=4,fig.width=6}
plot(varImp(modFit2),15)
```

 Features were pruned from the data set iteratively, removing those which were of low importance (0-3%), or highly correlated with other important features, and then repeating the process with a new set of models.Accuracy was compared at each step, to ensure that removing a feature did not degrade the model significantly. The feature set was eventually reduced to just 8 variables:  
ROLL_FOREARM  
PITCH_FOREARM  
ROLL_BELT  
PITCH_BELT  
YAW_BELT  
MAGNET_DUMBBELL_X  
MAGNET_DUMBBELL_Y  
MAGNET_DUMBBELL_Z  
This is clearly much reduced from the original 52 features, but still produced models that performed well on 10-20% of the training set.

```{r}
# Limit training data set to selected features
train <- select( train, roll_forearm, pitch_forearm, roll_belt, pitch_belt, yaw_belt,
                    magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z, classe)
supertrain <- select( supertrain, roll_forearm, pitch_forearm, roll_belt, pitch_belt, yaw_belt,
                    magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z, classe)

```

##Model Selection
The models considered in this step were random forest, boosted trees (GBM) and linear discriminant analysis (lda). Although classification trees were used during feature selection, the performance was poor, and they were not retained at this stage. Each model was trained using the full training set (60% of the data), the eight predictors listed above, and 5-fold cross-validation. The accuracy of each model was estimated by comparing the predicted classes for the test set with the actual values, using the confusion matrix. The run time for each model was also calculated. (The code used for each model can be found in the Appendix B).

####Option 1. Random Forest Model
```{r fitRF,message=FALSE,echo=FALSE}
# Code for creating the random forest model, and confusion matrix
start1 <- Sys.time()
set.seed(203)
modFitRF <- train(classe~., method="rf",data=train, trControl=controla)
round(Sys.time() - start1,2)
confusionMatrix(test$classe, predict(modFitRF,test))
```


####Option 2. Boosted Tree (GBM) Model
```{r fitGBM,message=FALSE,echo=FALSE}
## Code for creating the boosted tree model, and confusion matrix
gbmGrid <- expand.grid(n.trees = c(10,50, 100, 150, 200), 
                       interaction.depth = c(1,3,10), 
                       shrinkage = 0.1,
                       n.minobsinnode = 10)
start1 <- Sys.time()
set.seed(34987)
modFitGBM <- train(classe~., method="gbm",data=train, 
                   tuneGrid = gbmGrid,trControl=controla, verbose=FALSE)
round(Sys.time() - start1,2)
confusionMatrix(test$classe, predict(modFitGBM,test))
```


####Option 3. Linear Discriminant Analysis (LDA) Model
```{r fitLDA,message=FALSE,echo=FALSE}
# Code for creating the linear discriminant analysis model, and confusion matrix
start1 <- Sys.time()
set.seed(8734)
modFitLDA <- train(classe~., method="lda",data=train, trControl=controla)
round(Sys.time() - start1,2)
confusionMatrix(test$classe, predict(modFitLDA,test))
```



####Final Model Choice
From the confusion matrices above, the estimated accuracies (for predictions on the testing set) for each models were:

Model Type    | Accuracy (95% CI)  
------------- | -----------------  
Random Forest | 0.9762 - 0.9851  
GBM           | 0.9741 - 0.9835  
LDA           | 0.4566 - 0.4885

Based on these statistics, the __random forest model__ was selected. It is likely that the GBM model would perform equally well, since both the models achieved a high level of accuracy on this (artificially complete) data set. However, the random forest has the advantage that the caret package performs cross-validation and calculates OOB erros during the model-fitting process. The LDA model did not perform well, and was not considered as the final model.

##Fitting the Final Model
The final random forest model was built using  
* all the data except the set-aside validation set (80% of the total)  
* the eight predictors listed above  
* 5-fold cross-validation in the caret package
```{r}

controla <- trainControl(method = "cv", number=5 )
start1 <- Sys.time()
set.seed(2549)
modFinal <- train(classe~., method="rf",data=supertrain, trControl=controla)
round(Sys.time() - start1,2)
```

The model produced the following statistics:
```{r}
print(modFinal)
print(modFinal$finalModel)
```
Note the estimated error rate of 1.3% for out of sample errors, which caret has produced using cross-validation.

The majority of these errors (mis-classifications) come from classes 'B' and 'C'.

```{r plotRFinal,message=FALSE,echo=FALSE}
# Code for plotting errors for final random forest model
plot(modFinal$finalModel,main="Error Rates for Each Classe")
modFinal$finalModel.legend <- if (is.null(modFinal$finalModel$test$err.rate)) 
{colnames(modFinal$finalModel$err.rate)} else {colnames(modFinal$finalModel$test$err.rate)}
legend("top", cex =0.7, legend=modFinal$finalModel.legend, lty=c(1,2,3,4,5), col=c(1,2,3,4,5), horiz=T)
```


##Estimating Out of Sample Errors for the Final Model
The final step is to estimate the out of sample errors for the final model, using the untouched validation set to compare predicted values with actuals. (This is a second definition of cross-validation)
```{r CM, echo=FALSE}
# Code to print the confusion matrix for the validation set for final model
confusionMatrix(validation$classe, predict(modFinal,validation))
```

This gives an estimated out-of-sample error of 1.1% (i.e. 100% - an accuracy of 98.9%). This is consistent with the 1.3% OOB error rate calculated by the caret random forest algorithm using cross-validation.

##Prediction Assignment Submission
The final model was used to predict the classe variable for the 20 cases supplied in the "test" data for the class. All the predictions matched the actual classe variable. (100% accuracy on this data set).

```{r,eval=FALSE}
pmltest <- read.csv("pml-testing.csv", header=TRUE, na.strings = c("NA","#DIV/0!"))
predict(modFinal,pmltest)
```


##Conclusion
It is possible to determine the quality of an exercise using on-body sensors and a limited set of variables with 99% accuracy (an estimated out-of-sample error rate of 1.1%), despite the data quality problems and logic problems noted earlier. The final model was a random forest, using 8 variables, and 80% of the data.

#APPENDIX A - ADDITIONAL FIGURES

####Additional Plots Showing Relationships Between Predictors and Classe
```{r,echo=FALSE}
qplot(roll_belt,yaw_belt,colour=classe,data=dat2)
```

```{r,echo=FALSE}
qplot(magnet_dumbbell_y,magnet_dumbbell_z, colour=classe,data=dat2)
```

####Accuracies for Boosted Tree Model during Model Selection
```{r plotGBM,message=FALSE,echo=FALSE}
# Code for plotting accuracy for boosted tree model
plot(modFitGBM)
```

####Error Rates for Random Forest Model during Model Selection
```{r plotRF,message=FALSE,echo=FALSE}
# Code for plotting errors for random forest model
plot(modFitRF$finalModel,main="Error Rates for Each Classe")
modFitRF$finalModel.legend <- if (is.null(modFitRF$finalModel$test$err.rate)) 
{colnames(modFitRF$finalModel$err.rate)} else {colnames(modFitRF$finalModel$test$err.rate)}
legend("top", cex =0.7, legend=modFitRF$finalModel.legend, lty=c(1,2,3,4,5), col=c(1,2,3,4,5), horiz=T)
```


#APPENDIX B - SUPPORTING CODE 
```{r varImp, eval=FALSE}
```

```{r fitRF, eval=FALSE}
```

```{r plotRF, eval=FALSE}
```

```{r fitGBM, eval=FALSE}
```

```{r plotGBM, eval=FALSE}
```

```{r fitLDA, eval=FALSE}
```

```{r plotRFinal, eval=FALSE}
```

```{r CM, eval=FALSE}
```
